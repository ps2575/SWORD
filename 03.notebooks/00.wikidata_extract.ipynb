{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38780a4e-7fa5-432a-a9f3-f6ff9d6c6e0a",
   "metadata": {},
   "source": [
    "# 00. Wikidata Extract\n",
    "\n",
    "이 노트북은 Wikidata의 `latest-all.json.gz` 파일로부터 필요한 데이터를 추출하는 단계입니다.\n",
    "\n",
    "## 주요 단계\n",
    "1. JSON.gz line-by-line 스트리밍 파싱으로 추출\n",
    "2. Subjects / Properties / Triple 추출 (id 및 언어별 label + (sbj-prop-obj) 관계 추출)\n",
    "3. Chunk 단위로 Parquet 저장 (qid_labels_0001.parquet 등)\n",
    "4. 추출된 데이터는 `../00.data/00.wikidata/00.wikidata_templates/`에 저장\n",
    "\n",
    "## Wikidata 구조 (Subject - Property - Object)\n",
    "\n",
    "| 용어        | 의미                                                   |\n",
    "| --------- | ---------------------------------------------------- |\n",
    "| entity    | Subject(QID) 또는 Property(PID)                         |\n",
    "| claim     | property별 statement 묶음                                |\n",
    "| statement | 지식의 한 줄 (property + value + qualifiers + references) |\n",
    "| mainsnak  | statement의 핵심 값 (property + datavalue)                |\n",
    "| qualifier | statement의 조건                                         |\n",
    "| reference | 출처                                                    |\n",
    "| datavalue | object에 해당                                            |\n",
    "\n",
    "\n",
    "\n",
    "Entity  \n",
    "├── id  한국\n",
    "├── type  \n",
    "├── labels  \n",
    "├── descriptions  \n",
    "├── aliases  \n",
    "├── claims  \n",
    "│   ├── P01 (각 속성 ID)  (~의 속함)\n",
    "│   │   ├── mainsnak  \n",
    "│   │   │   ├── snaktype  \n",
    "│   │   │   ├── property  \n",
    "│   │   │   ├── datavalue  \n",
    "│   │   │   │   ├── value  -> qid (아시아)\n",
    "│   │   │   │   └── type  \n",
    "│   │   │   └── datatype   -> wikibase-entityid\n",
    "│   │   ├── type  \n",
    "│   │   ├── rank  \n",
    "│   │   ├── qualifiers  \n",
    "│   │   ├── qualifiers-order  \n",
    "│   │   ├── references  \n",
    "│   │   └── id  \n",
    "│   └── P02  \n",
    "├── sitelinks  \n",
    "│   ├── enwiki  \n",
    "│   │   ├── site  \n",
    "│   │   ├── title  \n",
    "│   │   └── badges  \n",
    "│   ├── frwiki  \n",
    "│   └── ...  \n",
    "└── lastrevid  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aba3726-6abf-48c0-ac58-94bc504ff05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import gzip\n",
    "import glob\n",
    "import uuid\n",
    "import psutil\n",
    "import shutil\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import orjson \n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d47e1-633e-4d5b-9548-a7786e843a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 선언\n",
    "DATA_DIR = '../00.data/00.wikidata/latest-all.json.gz'                  # Wikidata 원본 덤프\n",
    "SAVE_DIR = '../00.data/00.wikidata/00.wikidata_extract/'              # 최종본 Parquet 저장 위치\n",
    "TEMP_DIR = '../00.data/00.wikidata/00.wikidata_extract/_tmp'          # 임시 분할본 저장 위치\n",
    "os.makedirs(SAVE_DIR, exist_ok = True)\n",
    "os.makedirs(TEMP_DIR, exist_ok = True)\n",
    "\n",
    "# 사용 언어 8개\n",
    "LANGUAGE_LIST = ['en', 'fr', 'de', 'es', 'it', 'pt', 'ko', 'ja']\n",
    "\n",
    "# 병렬 세팅\n",
    "BATCH = 200000                          # 각 Worker 당 Entity line 수\n",
    "ROW_GROUP_ROWS = 200000                 # Parquet 저장 시 group row 수\n",
    "N_WORKERS = max(2, mp.cpu_count() - 2) # 사용할 Worker 수 (CPU 코어 개수에 따라)\n",
    "CHECK_EVERY = 10                       # Batch -> N개 마다 RAM 사용량 체크\n",
    "MAX_RAM_GB = 900                       # 최대 사용 RAM (현재 서버 max 1TB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f550c74-7849-4f7b-9bd2-b44d8416b365",
   "metadata": {},
   "source": [
    "# extract_entity 함수 \n",
    "## json 파싱 -> line 1 줄 (== wikidata entity 문서 1개) 씩 처리\n",
    "\n",
    "1. 입력된 entity == line 의 id 체크  \n",
    "   --> QID or PID 구분해서 처리\n",
    "2. QID 처리 (QID == Subject, QSUB = Object)  \n",
    "   QID 의 언어별 LABEL과 Q-P-Q 의 triples 추출 및 저장  \n",
    "   (datavalue.type == wikidata-entityid 일 때 (즉, Object Entity (QSUB) 가 존재하는 경우만 triples 에 추가)\n",
    "4. PID 처리 (PID == Property)\n",
    "   PID 의 언어별 LABEL 추출 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92557d06-4359-46ab-8027-57fd186943cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity 문서의 LABEL 중 사용할 Language 부분만 넘겨주는 함수 (아래의 Extract 함수 초반 부분에 바로 사용됨)\n",
    "def get_labels(labels_dict):\n",
    "    return {language: labels_dict.get(language, {}).get('value', \"I Don't Know!!\") for language in LANGUAGE_LIST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932da63-b78a-44ad-93cd-cb9759c71577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity(entity):\n",
    "    entity_id = entity.get('id') # 입력된 line 의 Entity ID 부터 파악\n",
    "    \n",
    "    if not entity_id or not isinstance(entity_id, str): # 파악 불가 시 None 반환 하고 종료 (매 Line 마다 체크용)\n",
    "        return None, None, None\n",
    "    \n",
    "    labels_dict = entity.get('labels', {}) # 현재 입력된 Entity 문서(QID 또는 PID)의 LABEL 딕셔너리\n",
    "\n",
    "    # QID: 즉, Subject 문서에서 LABELS 와 TRIPLES 추출하기\n",
    "    if entity_id.startswith('Q'): \n",
    "        # subject 라벨 구성\n",
    "        # 형태: {'subject': 'Qxxx', 'en': '...', 'ko': '...'}\n",
    "        subject_id = entity_id \n",
    "        subject_labels = {'subject':subject_id, **get_labels(labels_dict)}\n",
    "\n",
    "        # Q-P-Q 관계 추출하기 (json 구조는 맨 위 마크다운 클릭해서 참고 바람)\n",
    "        triples = []\n",
    "\n",
    "        # claims 에 포함된 Property ID, Object ID (QSUB) 파악하기 \n",
    "        # entity.get('claims') -> claims -> (property, statements) 로 구성됨\n",
    "        claims = entity.get('claims', {})\n",
    "        for property_id, statements in claims.items():\n",
    "            if not property_id.startswith('P'):  continue # PID 존재 확인\n",
    "            if not isinstance(statements, list): continue\n",
    "\n",
    "            # Property 정보인 Statements 를 타고 내려가면서 연결된 Object ID 를 찾아냄 \n",
    "            for statement in statements:\n",
    "                mainsnak = statement.get('mainsnak', {})\n",
    "                if not mainsnak.get('snaktype') == 'value': continue\n",
    "                \n",
    "                datavalue = mainsnak.get('datavalue', {})\n",
    "                if not datavalue.get('type') == 'wikibase-entityid':continue\n",
    "                \n",
    "                value = datavalue.get('value')\n",
    "                if not value: continue\n",
    "                \n",
    "                object_id = value.get('id') \n",
    "\n",
    "                # 간혹 value.get('id') 에서 못 찾으면 numeric-id 로 찾아서 직접 작성해서 넣기\n",
    "                if not object_id and value.get('entity-type') == 'item':\n",
    "                    numeric_id = value.get('numeric-id')\n",
    "                    if isinstance(numeric_id, int): object_id = f\"Q{numeric_id}\"\n",
    "\n",
    "                if object_id and isinstance(object_id, str) and object_id.startswith('Q'):\n",
    "                    triples.append((subject_id, property_id, object_id))\n",
    "\n",
    "        return subject_labels, None, triples or None\n",
    "    \n",
    "    # PID: 즉, Property 문서에서 LABELS 추출하기\n",
    "    elif entity_id.startswith(\"P\"):\n",
    "        property_id = entity_id\n",
    "        property_labels = {'property':property_id, **get_labels(labels_dict)}            \n",
    "        return None, property_labels, None\n",
    "\n",
    "        \n",
    "    return None, None, None\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfcdf87-abe3-48bb-a092-9c2aa135a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 Worker의 데이터 처리\n",
    "# 입력: lines -> Batch 크기 만큼의 json lines (Entity 문서들)\n",
    "# 위의 extract_entity() 함수 호출해서 각 문서마다 LABELS, TRIPLES 처리함\n",
    "# 그 다음 Subject_labels, Property_labels, Triples 리스트에 누적해서 Parquet으로 분할 저장함\n",
    "def _worker(lines):\n",
    "    os.makedirs(TEMP_DIR, exist_ok = True)\n",
    "\n",
    "    subjects_rows = []\n",
    "    properties_rows = []\n",
    "    triples_rows = []\n",
    "\n",
    "    for line in lines:\n",
    "        raw = line.rstrip()             # 마지막 개행 제거해서 처리 편하게\n",
    "        \n",
    "        if raw.endswith(b','):          # Wikidata 구조상 뒤에 ',' 가 붙는 경우 제거\n",
    "            raw = raw[:-1]\n",
    "        try:\n",
    "            entity = orjson.loads(raw)  # orjson이 빠르다고 함\n",
    "        except Exception:   \n",
    "            continue                    # json 오류 스킵\n",
    "\n",
    "        # 함수 호출해서 정보 추출하고\n",
    "        subjects, properties, triples = extract_entity(entity)\n",
    "        # 각 리스트에 추가한 뒤\n",
    "        if subjects: subjects_rows.append(subjects)\n",
    "        if properties: properties_rows.append(properties)\n",
    "        if triples: triples_rows.extend(triples)\n",
    "\n",
    "    # .parquet으로 저장할거임\n",
    "    written = []         # (kind, saved_path) 를 가진 리스트 | kind : 파일 종류 (Subject, Properies, Triples 중 하나)\n",
    "    uid = uuid.uuid4().hex # 고유 파일명 생성하는 부분\n",
    "\n",
    "    # QID_LABELS, PID_LABELS, TRIPLES 저장\n",
    "    if subjects_rows:\n",
    "        path = f\"{TEMP_DIR}/subjects_{uid}.parquet\"\n",
    "        pd.DataFrame(subjects_rows).to_parquet(path, \n",
    "                                               compression = 'snappy',\n",
    "                                               row_group_size = ROW_GROUP_ROWS,)\n",
    "        written.append(('subject', path))\n",
    "\n",
    "    if properties_rows:\n",
    "        path = f\"{TEMP_DIR}/properties_{uid}.parquet\"\n",
    "        pd.DataFrame(properties_rows).to_parquet(path,\n",
    "                                                 compression = 'snappy',\n",
    "                                                 row_group_size = ROW_GROUP_ROWS,)\n",
    "        written.append(('property', path))\n",
    "\n",
    "    if triples_rows:\n",
    "        path = f\"{TEMP_DIR}/triples_{uid}.parquet\"\n",
    "        pd.DataFrame(triples_rows, \n",
    "                     columns = ['subject', 'property', 'object']).to_parquet(path,\n",
    "                                                                             compression = 'snappy',\n",
    "                                                                             row_group_size = ROW_GROUP_ROWS,)\n",
    "        written.append(('triples', path))\n",
    "    \n",
    "    # worker 실행 결과: (kind, tmp_path) 목록 return\n",
    "    return written        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34e2fc-075b-42e4-8b4f-4c072d3c9a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE_DIR 내의 기존 parquet 파일 중,\n",
    "# kind_00001.parquet 형태의 숫자를 읽어서 다음 번호(next index)를 반환하는 함수\n",
    "\n",
    "def _current_idx():\n",
    "    # \"_00001.parquet\" 패턴 추출용 정규식\n",
    "    # 괄호 (…) 안에 있는 (\\d{5}) 가 group(1), 즉 숫자 5자리 추출\n",
    "    pat = re.compile(r'_(\\d{5})\\.parquet$') \n",
    "    nums = []\n",
    "\n",
    "    # SAVE_DIR 안에 있는 모든 파일 탐색\n",
    "    for file_name in os.listdir(SAVE_DIR):\n",
    "        m = pat.search(file_name)\n",
    "        if m:\n",
    "            nums.append(int(m.group(1)))\n",
    "\n",
    "    # nums 가 비어있지 않으면 가장 큰 번호 + 1 반환\n",
    "    # (즉 다음 저장될 파일의 index)\n",
    "    return max(nums) +  1 if nums else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f81442-1673-4644-9d8c-e366f4ef6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extract():\n",
    "    \"\"\"\n",
    "    Wikidata latest-all.json.gz 파일을 스트리밍으로 읽으면서\n",
    "    QID 라벨 / PID 라벨 / Triple 들을 청크 단위로 추출하여\n",
    "    Parquet 파일로 저장하는 최상위 함수.\n",
    "    \"\"\"\n",
    "    # 매번 실행 전에 임시 저장 폴더 초기화하는 부분\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors = True)\n",
    "    os.makedirs(TEMP_DIR, exist_ok = True)\n",
    "\n",
    "    # fork 방식으로 worker 프로세스 시작 (리눅스에서 빠름)\n",
    "    context = mp.get_context('fork')\n",
    "    pool = context.Pool(N_WORKERS, maxtasksperchild = 2)\n",
    "\n",
    "    # SAVE_DIR 안에 기존 parquet 파일들이 몇 번까지 저장됐는지 파악 → 다음 번호 구함\n",
    "    idx = mp.Value('i', _current_idx())\n",
    "    \n",
    "    inflight = [] # 아직 끝나지 않은 worker 작업(AsyncResult 객체) 리스트\n",
    "    batches = 0   # 지금까지 처리한 청크(batch) 개수 -> CHECK_EVERY 개수 마다 확인할거라 필요\n",
    "\n",
    "    # flush: 완료된 worker 결과(future)를 받아 최종 폴더로 rename \n",
    "    # (Future = Future 객체 (AsyncResult) 아직 실행 중이거나, 나중에 결과를 가져올 수 있는 통신 핸들러)\n",
    "    def _flush(queue, bar):\n",
    "        \"\"\"\n",
    "        queue(=inflight) 맨 앞에서부터 완료된 worker를 꺼내\n",
    "        worker가 생성한 tmp parquet 파일들을 SAVE_DIR로 옮기는 단계.\n",
    "        \"\"\"\n",
    "\n",
    "        # queue[0] 이 존재하고, ready() == True → 해당 worker 작업 끝남\n",
    "        while queue and queue[0].ready():\n",
    "            # 작업 끝난 worker 하나 pop\n",
    "            # worker 결과는:  [('subject', tmp_path), ('property', tmp_path2)...]\n",
    "            future = queue.pop(0)\n",
    "            for kind, tmp_path in future.get():\n",
    "                with idx.get_lock():                          # index는 여러 프로세스가 공유하는 값이므로 lock 필요\n",
    "                    seq = idx.value\n",
    "                    idx.value += 1 \n",
    "                    \n",
    "                final = f\"{SAVE_DIR}/{kind}_{seq:05}.parquet\" # 최종 저장될 parquet 경로 \n",
    "                os.rename(tmp_path, final)                    # tmp 파일을 최종 위치로 이동\n",
    "                bar.write(f\"Saved {os.path.basename(final)}\") # tqdm 출력\n",
    "\n",
    "    # gz 파일 라인 단위 스트리밍 읽기 시작\n",
    "    with gzip.open(DATA_DIR, 'rb') as gz, tqdm(unit = 'ent', mininterval = 4) as bar:\n",
    "        buffer = []  # BATCH = 50,000 라인 모을 버퍼 (여기서만 라인 누적함)\n",
    "\n",
    "        for line in gz:\n",
    "            if line and line[0] == 123:                               # Wikidata JSON 구조: entity 시작은 '{' (ASCII 123)\n",
    "                buffer.append(line)                                   # → line[0] == 123 일 때만 유효 entity line\n",
    "\n",
    "            \n",
    "            if len(buffer) >= BATCH:                                  # 버퍼가 청크(batch) 기준량에 도달하면 → worker 에게 넘김\n",
    "                inflight.append(pool.apply_async(_worker, (buffer,))) # 비동기로 worker 호출 → inflight에 future 저장\n",
    "                buffer = []\n",
    "                batches += 1\n",
    "\n",
    "                _flush(inflight, bar)                                 # 끝난 worker 있으면 바로 flush\n",
    "\n",
    "                if batches % CHECK_EVERY == 0:                        # 일정 batch마다 메모리 점검\n",
    "                    while psutil.virtual_memory().percent > 90:       # RAM 90% 이상 되면 작업 속도 조절\n",
    "                        _flush(inflight, bar)\n",
    "                        time.sleep(1)\n",
    "            bar.update(1)\n",
    "\n",
    "        # gz 읽기 종료 후 마지막 buffer 처리 (BATCH 사이즈 다 못채운 마지막 버퍼 존재할 경우 처리)\n",
    "        if buffer:\n",
    "            inflight.append(pool.apply_async(_worker, (buffer,)))\n",
    "            _flush(inflight, bar)\n",
    "\n",
    "        # 남아있는 inflight worker들 전부 flush\n",
    "        for future in inflight:        \n",
    "            for kind, tmp_path in future.get():\n",
    "                with idx.get_lock():\n",
    "                    seq = idx.value\n",
    "                    idx.value += 1\n",
    "                final = f\"{SAVE_DIR}/{kind}_{seq:05}.parquet\"\n",
    "                os.rename(tmp_path, final)\n",
    "                bar.write(f\"Saved {os.path.basename(final)}\")\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    logging.info(f\"Extraction finished - Total files: {idx.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912c62a-b678-4c66-941d-3cbcf2efb825",
   "metadata": {},
   "source": [
    "# 분할된 Parquet 병합 코드 \n",
    "## Subject / Property / Triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc94516-0ad8-401f-9066-b08c41c63085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a8028-8f51-4a34-b36b-8d7b56f3bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_sort(input_dir, prefix, output_path, sort_by):\n",
    "    \"\"\"\n",
    "    pyarrow.dataset 을 사용한 완전 스트리밍 병합.\n",
    "    \n",
    "    메모리를 거의 안 쓰고, row group 단위로 병합함.\n",
    "\n",
    "    + ID 기준 정렬해서 저장\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = os.path.join(input_dir, f\"{prefix}*.parquet\")\n",
    "    files = sorted(glob.glob(pattern))\n",
    "\n",
    "    if not files:\n",
    "        print(f\"[WARN] No files found for '{prefix}'\")\n",
    "        return\n",
    "\n",
    "    dataset = ds.dataset(files, format=\"parquet\")\n",
    "\n",
    "    # 메모리 최적: table로 바로 변환\n",
    "    table = dataset.to_table()\n",
    "\n",
    "    # pandas 변환\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    df = df.sort_values(sort_by).reset_index(drop=True)\n",
    "\n",
    "    df.to_parquet(output_path, compression='snappy')\n",
    "    print(f\"{prefix} merged & sorted → {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27524e4a-f48b-462c-b1fe-6b45833d694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_DIR = '../00.data/00.wikidata/00.wikidata_extract/'              # 최종본 Parquet 저장 위치\n",
    "MERGED_DIR = '../00.data/00.wikidata/01.wikidata_merged/'\n",
    "os.makedirs(MERGED_DIR, exist_ok = True)\n",
    "\n",
    "merge_and_sort(EXTRACT_DIR, \"subject\", f\"{MERGED_DIR}subject.parquet\", \"subject\")\n",
    "merge_and_sort(EXTRACT_DIR, \"property\", f\"{MERGED_DIR}property.parquet\", \"property\")\n",
    "merge_and_sort(EXTRACT_DIR, \"triples\", f\"{MERGED_DIR}triples.parquet\", [\"subject\", \"property\", \"object\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0108957-1fe5-41ca-9101-a2b6affbe385",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df = pq.read_table(MERGED_DIR+'/subject.parquet').to_pandas()\n",
    "property_df = pq.read_table(MERGED_DIR+'/property.parquet').to_pandas()\n",
    "triples_df = pq.read_table(MERGED_DIR+'/triples.parquet').to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b5e40-6a8f-423a-bb62-208b796655de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_valid = ~(subject_df[LANGUAGE_LIST] == \"I Don't Know!!\").any(axis = 1)\n",
    "subject_df = subject_df[mask_valid].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10e6fa-7963-4e1b-b062-96ce4c78f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_valid = ~(property_df[LANGUAGE_LIST] == \"I Don't Know!!\").any(axis=1)\n",
    "property_df = property_df[mask_valid].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7199ea-75f3-44d9-a669-c948e125c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df_filtered = subject_df.set_index(\"subject\", drop=True)\n",
    "property_df_filtered = property_df.set_index(\"property\", drop=True)2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d24d81-bd8c-4d00-888b-e712f78e1939",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df_filtered.to_parquet(f\"{MERGED_DIR}subject_filtered.parquet\", compression = \"snappy\", index = True)\n",
    "subject_df_filtered.to_parquet(f\"{MERGED_DIR}property_filtered.parquet\", compression = \"snappy\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766116f3-b8a0-4e76-82d3-234ecbed4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d562e21-29c8-4aea-a74c-2cc5e3dae6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2a19b-275c-485a-a55b-90dd82fdd923",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_df = triples_df.sort_values([\"subject\", \"property\", \"object\"]).reset_index(drop=True)\n",
    "triples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da07c3b-5fd7-4b39-9b69-2604bc4fe801",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_df.to_parquet(f\"{MERGED_DIR}triples_filtered.parquet\", compression = \"snappy\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d4ea7-8693-4751-b48b-647f0b2fe0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_df = triples_df.sort_values([\"subject\", \"property\", \"object\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf9c45a-18eb-4949-8058-a9e3960dde6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df = pq.read_table(MERGED_DIR+'/subject.parquet').to_pandas()\n",
    "\n",
    "# LANGUAGE_LIST 기준으로 모든 언어 값이 동일한 행 찾기\n",
    "mask_same = subject_df[LANGUAGE_LIST].nunique(axis=1) == 1\n",
    "\n",
    "# 해당 행 출력\n",
    "subject_same = subject_df[mask_same]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b9f90-ceaa-4521-b635-8396588a3a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikibench",
   "language": "python",
   "name": "wikibench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
