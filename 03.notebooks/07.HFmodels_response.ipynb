{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db67f2-9f2f-481c-b3c5-2edb76699b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from accelerate import Accelerator, DataLoaderConfiguration\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47380f7d-6321-4341-a430-5ba277bec2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_DIR = '../00.data/00.wikidata/03.wikidata_template/'\n",
    "TEMPLATE_NAME = ['00.original_template_500.parquet',\n",
    "                 '01.subject_shuffled_template_500.parquet',\n",
    "                 '02.object_shuffled_template_500.parquet',\n",
    "                 '03.property_scoped_subject_shuffled_template_500.parquet',\n",
    "                 '04.property_scoped_object_shuffled_template_500.parquet']\n",
    "\n",
    "MODEL_DIR = '../01.models/'\n",
    "MODEL_NAME = ['Meta/meta-llama/Llama-3.1-8B-Instruct/',\n",
    "              'Mistral/mistralai/Mistral-Nemo-Instruct-2407/',\n",
    "              'Qwen/Qwen3-8B/',\n",
    "              'Qwen/Qwen3-14B/']\n",
    "RESPONSE_DIR = '../00.data/01.model_response/01.HF_Models_response/'\n",
    "\n",
    "LANGUAGE_LIST = ['en', 'fr', 'de', 'es', 'it', 'pt', 'ko', 'ja']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2268007-fdfc-49e5-81da-16ff6dcb713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_0 = pq.read_table(f\"{TEMPLATE_DIR}{TEMPLATE_NAME[0]}\").to_pandas()\n",
    "template_1 = pq.read_table(f\"{TEMPLATE_DIR}{TEMPLATE_NAME[1]}\").to_pandas()\n",
    "template_2 = pq.read_table(f\"{TEMPLATE_DIR}{TEMPLATE_NAME[2]}\").to_pandas()\n",
    "template_3 = pq.read_table(f\"{TEMPLATE_DIR}{TEMPLATE_NAME[3]}\").to_pandas()\n",
    "template_4 = pq.read_table(f\"{TEMPLATE_DIR}{TEMPLATE_NAME[4]}\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4e34a-7afe-4843-915e-c82e8be94d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference(prompts, batch_size = 100):\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i: i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            return_tensors = 'pt', \n",
    "            padding = True, \n",
    "            truncation = True, \n",
    "            max_length = 128).to(accelerator.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens = 100,\n",
    "                num_return_sequences = 1,\n",
    "                do_sample = True, \n",
    "                temperature = 0.1,\n",
    "                # attention_mask = inputs['attention_mask'],\n",
    "                pad_token_id = tokenizer.pad_token_id,\n",
    "                eos_token_id = tokenizer.eos_token_id,\n",
    "                repetition_penalty = 1.3,\n",
    "                return_dict_in_generate = True\n",
    "            )\n",
    "            \n",
    "        gen_only = outputs.sequences[:, inputs[\"input_ids\"].shape[1]:]\n",
    "        decoded = tokenizer.batch_decode(gen_only, skip_special_tokens=True)\n",
    "        results.extend(decoded)\n",
    "    return results\n",
    "\n",
    "def generate_responses(df, prefix, out_file = None):\n",
    "    total_start = time.time()\n",
    "    for lang in LANGUAGE_LIST:\n",
    "        lang_start = time.time()\n",
    "        prompt_col = f\"prompt_{lang}\"\n",
    "        response_col = f\"response_{lang}\"\n",
    "        print(f\"Inference {prefix}: {lang} \")\n",
    "\n",
    "        prompts = df[prompt_col].astype(str).tolist()\n",
    "        if len(prompts) == 0:\n",
    "            continue\n",
    "\n",
    "        responses = batch_inference(prompts, batch_size = 6)\n",
    "        df[response_col] = responses\n",
    "        lang_elapsed = time.time() - lang_start\n",
    "        print(f\"Time -> {lang}:{lang_elapsed:.2f} 초\")\n",
    "    pq.write_table(pa.Table.from_pandas(df), out_file)\n",
    "    print(\"저장 완료:\", out_file)\n",
    "    total_elapsed = time.time() - total_start\n",
    "    print(f\"총 소요 시간: {total_elapsed:.2f} 초\")\n",
    "    return df\n",
    "\n",
    "def unload_model():\n",
    "    global model, tokenizer, accelerator\n",
    "    try:\n",
    "        del model\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del accelerator\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    import gc, torch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    print(\"✓ Model fully unloaded (as much as possible without restarting kernel).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207dbb9-7454-4fd2-b05a-890296cd215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01924a6-e47e-402b-a1b5-e27699fefbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be11dd-5d1d-46ba-ab2e-e642bb7c9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_model()\n",
    "i = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR + MODEL_NAME[i])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_DIR + MODEL_NAME[i],\n",
    "        device_map = {'':[0,1]},\n",
    "    )\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b78a29-d04e-4bc0-b812-62c7b27d3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    unload_model()\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5'\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR + MODEL_NAME[i])\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_DIR + MODEL_NAME[i],\n",
    "            device_map = 'auto',\n",
    "        )\n",
    "    accelerator = Accelerator()\n",
    "    model = accelerator.prepare(model)\n",
    "\n",
    "    os.makedirs(RESPONSE_DIR + MODEL_NAME[i], exist_ok=True)\n",
    "    MODEL_OUTPUT_DIR = RESPONSE_DIR + MODEL_NAME[i]\n",
    "    copy_0 = template_0.copy()\n",
    "    copy_1 = template_1.copy()\n",
    "    copy_2 = template_2.copy()\n",
    "    copy_3 = template_3.copy()\n",
    "    copy_4 = template_4.copy()\n",
    "\n",
    "    copy_0 = generate_responses(copy_0, prefix=MODEL_NAME[i], out_file = MODEL_OUTPUT_DIR + \"00.original_response_100.parquet\")\n",
    "    copy_1 = generate_responses(copy_1, prefix=MODEL_NAME[i], out_file = MODEL_OUTPUT_DIR + \"01.subject_shuffled_response_100.parquet\")\n",
    "    copy_2 = generate_responses(copy_2, prefix=MODEL_NAME[i], out_file = MODEL_OUTPUT_DIR + \"02.object_shuffled_response_100.parquet\")\n",
    "    copy_3 = generate_responses(copy_3, prefix=MODEL_NAME[i], out_file = MODEL_OUTPUT_DIR + \"03.property_scoped_subject_shuffled_response_100.parquet\")\n",
    "    copy_4 = generate_responses(copy_4, prefix=MODEL_NAME[i], out_file = MODEL_OUTPUT_DIR + \"04.property_scoped_object_shuffled_response_100.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b34a0d-5de6-40af-80ac-64fa422ad2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_model()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR + MODEL_NAME[i])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_DIR + MODEL_NAME[i],\n",
    "        device_map = 'auto',\n",
    "    )\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare(model)\n",
    "\n",
    "os.makedirs(RESPONSE_DIR + MODEL_NAME[i], exist_ok=True)\n",
    "MODEL_OUTPUT_DIR = RESPONSE_DIR + MODEL_NAME[i]\n",
    "copy_0 = template_0.copy()\n",
    "copy_1 = template_1.copy()\n",
    "copy_2 = template_2.copy()\n",
    "copy_3 = template_3.copy()\n",
    "copy_4 = template_4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb87c0-fe5b-4352-a9ec-a01c259b441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3d2fb-f9b8-49ef-841f-02ab2303903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_0 = generate_responses(copy_0, prefix=MODEL_NAME[i], out_file = MODEL_OUTPUT_DIR + \"00.original_response_100.parquet\")\n",
    "copy_1 = generate_responses(copy_1, prefix=MODEL_NAME[i], out_file = MODEL_OUTPUT_DIR + \"01.subject_shuffled_response_100.parquet\")\n",
    "copy_2 = generate_responses(copy_2, prefix=MODEL_NAME[i], out_file = MODEL_OUTPUT_DIR + \"02.object_shuffled_response_100.parquet\")\n",
    "copy_3 = generate_responses(copy_3, prefix=MODEL_NAME[i], out_file = MODEL_OUTPUT_DIR + \"03.property_scoped_subject_shuffled_response_100.parquet\")\n",
    "copy_4 = generate_responses(copy_4, prefix=MODEL_NAME[i], out_file = MODEL_OUTPUT_DIR + \"04.property_scoped_object_shuffled_response_100.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikibench",
   "language": "python",
   "name": "wikibench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
